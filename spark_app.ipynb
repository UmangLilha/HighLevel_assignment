{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(worker_cores=1):\n",
    "    \"\"\"\n",
    "    - Initialize a new Spark session with a dynamic number of worker threads.\n",
    "    - Before creating a new session, stop the old one (if it exists).\n",
    "    - Return the SparkSession object.\n",
    "    \"\"\"\n",
    "    global spark\n",
    "\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"spark_model_training\")\n",
    "             .master(f\"local[{worker_cores}]\") \n",
    "             .getOrCreate())\n",
    "    \n",
    "    return spark\n",
    "\n",
    "def load_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Load data from a Parquet file using Spark.\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(file_path)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, feature_columns):\n",
    "    \"\"\"\n",
    "    Preprocess the data by assembling features into a single column.\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "    df = assembler.transform(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def train_gbt_with_cv(processed_df):\n",
    "    \"\"\"\n",
    "    Train a Gradient-Boosted Tree Regressor using Spark's GBTRegressor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # sample 50% of the data as the kernel was getting killed due to memory issues\n",
    "    sampled_df = processed_df.sample(fraction=0.5, seed=42)\n",
    "\n",
    "    # Using  GBTRegressor model\n",
    "    gbt = GBTRegressor(featuresCol='features',\n",
    "                       labelCol='Impact')\n",
    "    \n",
    "    # Using MAE instead of MAPE as MAPE was not available in RegressionEvaluator.\n",
    "    evaluator = RegressionEvaluator(labelCol='Impact', predictionCol='prediction', metricName='mae')\n",
    "\n",
    "    # Defining single set of parameters for the model as parameter tuning was not causing memory issues.\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [6])  # Maximum depth of each tree\n",
    "             .addGrid(gbt.maxIter, [500])  # Number of boosting iterations\n",
    "             .addGrid(gbt.stepSize, [0.1])  # Learning rate (step size)\n",
    "             .addGrid(gbt.subsamplingRate, [0.7])  # Subsampling rate (for each tree)\n",
    "             .addGrid(gbt.featureSubsetStrategy, [ 'onethird'])  # Subsample features\n",
    "             .build())\n",
    "\n",
    "    crossval = CrossValidator(estimator=gbt,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=3\n",
    "                              )  \n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cv_model = crossval.fit(sampled_df)\n",
    "\n",
    "    # Calculate training time\n",
    "    total_training_time = time.time() - start_time\n",
    "\n",
    "   \n",
    "    avg_mae_cv = np.mean(cv_model.avgMetrics)\n",
    "    \n",
    "\n",
    "    \n",
    "    return avg_mae_cv, total_training_time\n",
    "\n",
    "def run_experiment(worker_cores, file_path):\n",
    "    \"\"\"\n",
    "    Run the experiment with the specified number of worker cores and file path.\n",
    "    \"\"\"\n",
    "    spark = get_spark_session(worker_cores)\n",
    "    print(\"worker cores: \", spark.sparkContext.defaultParallelism)\n",
    "    df = load_data(spark, file_path).drop('main_author_encoded')\n",
    "    df = df.filter(df.Impact != 0)\n",
    "    df = df.drop('__index_level_0__')\n",
    "    total_features = df.columns\n",
    "    total_features.remove('Impact')\n",
    "    \n",
    "    preprocessed_df = preprocess_data(df, total_features)\n",
    "    \n",
    "    avg_mae_cv, total_training_time = train_gbt_with_cv(preprocessed_df)\n",
    "    \n",
    "    return avg_mae_cv, total_training_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = \"data_preprocessed.parquet\"\n",
    "\n",
    "\n",
    "\n",
    "worker_configs = [1, 2, 4]\n",
    "results = {}\n",
    "\n",
    "# Run the experiment for each number of workers\n",
    "\n",
    "for workers in worker_configs:\n",
    "    avg_mae_cv, total_training_time = run_experiment(workers, file_path)\n",
    "    results[workers] = (avg_mae_cv, total_training_time)\n",
    "    print(f\"Workers: {workers}, MAE: {avg_mae_cv}, Training Time: {total_training_time}\")\n",
    "\n",
    "print(\"Experiment Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results: {1: (46.39529514276564, 1184.555340051651), 2: (46.39529514276564, 1356.3587908744812), 4: (46.59486072405618, 1548.2583179473877)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belvilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
